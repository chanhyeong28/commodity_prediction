# Unified Configuration for Commodity Prediction System
# This configuration file manages all system components in one place

# System-wide settings
system:
  name: "Commodity Prediction System"
  version: "1.0.0"
  debug: false
  log_level: "INFO"
  device: "auto"  # auto, cpu, cuda, cuda:0, etc.

# Data configuration
data:
  raw_path: "data/raw"
  processed_path: "data/processed"
  train_file: "train.csv"
  test_file: "test.csv"
  target_pairs_file: "target_pairs.csv"
  train_labels_file: "train_labels.csv"
  
  # Data preprocessing
  preprocessing:
    fill_method: "forward_backward"  # forward, backward, forward_backward, interpolate
    normalize: true
    remove_outliers: false
    outlier_threshold: 3.0

# Patch configuration
patches:
  window_sizes: [7, 14, 28]  # Multiple window sizes for dynamic patches
  strides: [1, 3, 7]         # Stride options for overlapping patches
  embedding_dim: 128         # Embedding dimension for patches
  max_patches_per_series: 50 # Maximum patches per time series
  normalize_patches: true    # Normalize patch values
  min_patch_length: 5        # Minimum patch length
  
  # Patch embedder configuration
  embedder:
    architecture: "cnn"      # cnn, transformer, linear
    dropout: 0.1
    activation: "relu"

# Knowledge Graph configuration
knowledge_graph:
  db_path: "database/commodity_kg.db"
  dev_db_path: "database/dev_commodity_kg.db"
  
  # Patch settings for KG
  window_sizes: [7, 14, 28]
  strides: [1, 3, 7]
  
  # Graph construction
  correlation_threshold: 0.1
  p_value_threshold: 0.1
  extract_entities: true
  include_regime_detection: false
  
  # GraphRAG retrieval
  retrieval:
    max_nodes: 64
    max_edges: 128
    similarity_threshold: 0.1
    context_dim: 64

# Model configuration
model:
  # Time-LlaMA architecture
  d_model: 128
  n_layers: 4
  n_heads: 4
  d_ff: 512
  dropout: 0.1
  
  # GraphRAG context integration
  d_context: 64
  n_context_heads: 2
  max_context_tokens: 256
  
  # Output configuration
  n_targets: 80
  forecast_horizons: [1, 2, 3, 4]
  
  # PEFT/LoRA configuration
  use_peft: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05

# Training configuration
training:
  # Basic training settings
  batch_size: 32
  learning_rate: 1e-3
  weight_decay: 1e-8
  num_epochs: 100
  early_stopping_patience: 20
  
  # Data settings
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  
  # Optimization
  optimizer: "adamw"
  scheduler: "cosine"
  warmup_steps: 100
  
  # Monitoring
  log_interval: 10
  save_interval: 50
  eval_interval: 10

# Inference configuration
inference:
  # Batch processing
  batch_size: 64
  num_workers: 4
  
  # Prediction settings
  num_samples: 100
  confidence_level: 0.95
  
  # Output settings
  output_format: "csv"
  save_predictions: true
  save_uncertainty: true

# Environment-specific overrides
environments:
  development:
    system:
      debug: true
      log_level: "DEBUG"
    data:
      raw_path: "data/raw"
    knowledge_graph:
      db_path: "database/dev_commodity_kg.db"
    training:
      batch_size: 16
      num_epochs: 10
      early_stopping_patience: 5
  
  production:
    system:
      debug: false
      log_level: "INFO"
    training:
      batch_size: 64
      num_epochs: 200
      early_stopping_patience: 30
  
  kaggle:
    system:
      device: "cuda"
    training:
      batch_size: 32
      num_epochs: 50
    inference:
      batch_size: 128
      num_workers: 2
