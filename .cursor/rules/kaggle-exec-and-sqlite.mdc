---
alwaysApply: true
description: Kaggle execution constraints, SQLite loading, and inference-time policies
---

# Kaggle Execution + SQLite Rules

Operational rules for running in Kaggle (GPU ≤ 16GB, RAM ~29GB, Disk ~57GB, no internet, ≤8–9h wall time). Use a pre-built SQLite KG uploaded as a dataset. Load into memory as needed (NetworkX), do not persist back.

## Runtime Constraints

- Internet disabled. All assets must be in input datasets or the notebook filesystem.
- Time limit: ≤8h (train phase), ≤9h (forecast phase). Design for ≤6h typical.
- GPU memory ~16GB. Prefer FP16 and adapter-based fine-tuning.
- RAM ~29GB. Avoid fully materializing huge graphs in memory.

## SQLite Use

- Load a single `commodity_kg.db` from Kaggle input datasets.
- Tables expected: `ts_nodes`, `ts_edges`, `entities` (see graph rule for schema).
- Queries must:
  - Filter by `series_id` and `window_end` first (indexed fields),
  - Limit rows early, then do cosine similarity in Python/NumPy for candidates.
- Do not write back to the DB. Session is ephemeral.

## NetworkX Usage

- Build an in-memory subgraph only for current batch using fetched rows from SQLite.
- Optional centrality for ranking; avoid O(N^2) operations on large graphs.
- Free references between batches to control RAM.

## Forecasting-Only Inference

- No KG updates during inference. The KG is constructed offline and shipped with the notebook as input data.
- At inference time:
  1) Read `test.csv` slice from Kaggle API.
  2) For each forecasting date_id and target, retrieve a compact subgraph.
  3) Run the Time-LlaMA-style adapter model with KG context to produce predictions.
  4) Write predictions to the required submission format.

## Data Contracts

- Files present in Kaggle input:
  - `train.csv`, `train_labels.csv`, `test.csv`, `target_pairs.csv`, `lagged_test_labels/test_labels_lag_[1-4].csv`, `commodity_kg.db`.
- Column contracts align with competition spec. Use `target_pairs.csv` to parse targets and lags.

## Performance Targets & Tips

- Aim for ≤300ms per inference step per target batch on GPU, amortized.
- Cache per-date KG retrievals when multiple targets share context.
- Down-project KG embeddings to 64 dims before cross-attention.
- Use smaller batches for models if VRAM fragmentation occurs.

## Failure & Fallbacks

- If KG retrieval exceeds budget, reduce context nodes to ≤128.
- If GPU OOM, reduce: heads, layers, D; or drop context for low-importance targets.
- If time overruns, skip dynamic windows (use 7-day only) and disable centrality.

